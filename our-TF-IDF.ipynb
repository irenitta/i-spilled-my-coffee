{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "567e47a4376e425cbbf9dbc749bd0002",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1699781110454,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ff0ea424d9d54cbc9b725f6366224a69",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 0. Introduction\n",
    "\n",
    "Typically, comparing movie overviews is challenging due to the complexity and variability of language. To effectively measure similarity between movie overviews, we first need to transform them into a structured numerical format. This is where Term Frequency-Inverse Document Frequency ([TF-IDF](https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_832)) algorithm comes into play.\n",
    "\n",
    "In this notebook, we delve into the implementation and step-by-step explanation of the TF-IDF algorithm.\n",
    "\n",
    "*TF-IDF, standing for Term Frequency-Inverse Document Frequency, is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It is calculated by multiplying two metrics: how many times a word appears in a document (term frequency) and the inverse document frequency of the word across a set of documents. This approach diminishes the weight of commonly used words and amplifies that of unique terms, providing a more meaningful representation of text content.*\n",
    "\n",
    "Through this notebook, we will compute TF-IDF vectors for movie overviews (documents). This process will yield a matrix where each column represents a unique word in our corpus (all the words appearing in at least one movie overview), and each row represents an individual movie. This structured representation is essential for our recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ba8512a4b0f646a29c7fbc32fe0b0ffe",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1. Movie overview exploration\n",
    "First, let us inspect the plots of a few movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "cd234f5641fa4d8b99a2bdabe2b61dd6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 678,
    "execution_start": 1699781110455,
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9      James Bond must unmask the mysterious head of ...\n",
       "68     Craig and Smokey are two guys in Los Angeles h...\n",
       "69     Seth Gecko and his younger brother Richard are...\n",
       "153    Auggie runs a small tobacco shop in Brooklyn, ...\n",
       "178    Power up with six incredible teens who out-man...\n",
       "Name: overview, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the movies metadata CSV file\n",
    "movies_metadata = pd.read_csv('the-movies-dataset/movies_metadata.csv', low_memory=False, encoding='utf-8').dropna()\n",
    "\n",
    "# Print plot overviews of the first 5 movies\n",
    "movies_metadata['overview'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "db1aa09325c84325880051c095473d97",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 2. Preprocessing and Cleaning Overviews\n",
    " \n",
    "To enhance the quality of our TF-IDF analysis, it is crucial to preprocess and clean the movie overviews. This involves removing \"stop words\", which are commonly used words that do not contribute significantly to the overall meaning of the text (e.g., 'and', 'the', 'is'), and punctuation, as it is not informative for our analysis and can interfere with word comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "0503d149429a4845b717e8a72e8be8ee",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1699781133971,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Define the set of stop words to be excluded from the analysis\n",
    "stop_words = set([\n",
    "    # List of common stop words (ensure no duplicates and all necessary words are included)\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at',\n",
    "    'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n",
    "    'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'arent', \n",
    "    'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', '', 'andy', 'such', 'just',\n",
    "    'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself',\n",
    "    'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "    're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such',\n",
    "    't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too',\n",
    "    'under', 'until', 'up',\n",
    "    've', 'very',\n",
    "    'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\",\n",
    "    'wouldn', \"wouldn't\",\n",
    "    'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves',   'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",\n",
    "    \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him',\n",
    "    'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
    "    'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who',\n",
    "    'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
    "    'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    "    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "    'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
    "    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "    's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd',\n",
    "    'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn',\n",
    "    \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "    'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "    'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won',\n",
    "    \"won't\", 'wouldn', \"wouldn't\"\n",
    "])\n",
    "\n",
    "# Create a translation table to remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Initialize a set to hold all the unique cleaned words\n",
    "cleaned_unique_words = set()\n",
    "\n",
    "# Iterate over each overview to populate cleaned_unique_words\n",
    "for overview in movies_metadata['overview']:\n",
    "    # Clean the overview of punctuation and split into words\n",
    "    cleaned_words = [word.lower().translate(translator) for word in overview.split()]\n",
    "    # Update the set with words from this overview, excluding stopwords\n",
    "    cleaned_unique_words.update(word for word in cleaned_words if word not in stop_words)\n",
    "\n",
    "# cleaned_unique_words now contains all the unique words after removing stopwords and punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "145fb4780fa042e28b0f09997a42b888",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 3. Word frequency count \n",
    "\n",
    "After cleaning and preprocessing the movie overviews, our next step is to count the frequency of each word in these overviews. This is a crucial step for TF-IDF as it helps us understand the term frequency component. We create a list of dictionaries, where each dictionary corresponds to a movie overview, and the keys are the unique cleaned words with their frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "aa51b618ab9848e6958d450c5d544185",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "word_dicts = []\n",
    "\n",
    "# Count the word frequency for each overview\n",
    "for overview in movies_metadata['overview']:\n",
    "   # Split the overview into words, clean them, and filter out stopwords\n",
    "    words = [word.lower().translate(translator) for word in overview.split() if word.lower().translate(translator) not in stop_words]\n",
    "    # Initialize the dictionary for this overview with all cleaned unique words\n",
    "    word_dict = dict.fromkeys(cleaned_unique_words, 0)\n",
    "    # Count the words in the current line\n",
    "    for word in words:\n",
    "        word_dict[word] += 1\n",
    "    # Add the word count dictionary to the list of dictionaries\n",
    "    word_dicts.append(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3c5a06ee69b04f99b8e62551c5e2132a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Let us inspect the `word_dicts` list. We inspect the frequency of words corresponding to the first and second movie overview, i.e. `word_dicts[0]` and `word_dicts[1]`. The output we are seeing is truncated because it is too long, but if we look closely, we notice that it is showing word counts (mostly zeros since most words do not appear in every single overview).\n",
    "\n",
    "Let us inspect the frequency of words corresponding to the first and second movie overview, i.e. `word_dicts[0]` and `word_dicts[1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "468356fa68784e9ca3d49658817a70c8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 27,
    "execution_start": 1699782777038,
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in the first overview: [('goldeneye', 1), ('leader', 1), ('weapons', 1), ('utilizing', 1), ('syndicate', 1), ('system', 1), ('revenge', 1), ('bond', 1), ('britain', 1), ('janus', 1)]\n",
      "Top words in the second overview: [('something', 1), ('smoking', 1), ('craig', 1), ('afternoon', 1), ('hanging', 1), ('smokey', 1), ('angeles', 1), ('friday', 1), ('drinking', 1), ('guys', 1)]\n",
      "Length of the first dictionary: 8025\n",
      "Length of the second dictionary: 8025\n"
     ]
    }
   ],
   "source": [
    "first_dict = word_dicts[0]  \n",
    "second_dict = word_dicts[1]  \n",
    "\n",
    "# Function to get the top N words with the highest frequency\n",
    "def get_top_n_words(word_dict, n=10):\n",
    "    return sorted(word_dict.items(), key=lambda item: item[1], reverse=True)[:n]\n",
    "\n",
    "print(\"Top words in the first overview:\", get_top_n_words(first_dict))\n",
    "print(\"Top words in the second overview:\", get_top_n_words(second_dict))\n",
    "\n",
    "print(\"Length of the first dictionary:\", len(first_dict))\n",
    "print(\"Length of the second dictionary:\", len(second_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us analyze the word frequency distribution across all movie overviews. We will calculate the total word count for each overview and then sum these counts to understand the overall word usage in our movies dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "33a6668591ae4d7683d77070bf755e29",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count per overview: [18, 14, 30, 31, 24, 23, 33, 33, 14, 36, 23, 39, 34, 26, 33, 27, 14, 14, 37, 32, 37, 31, 23, 21, 11, 33, 30, 40, 20, 25, 51, 40, 38, 34, 18, 29, 30, 18, 48, 59, 29, 22, 20, 42, 34, 38, 35, 12, 20, 29, 15, 32, 22, 40, 9, 14, 29, 30, 18, 29, 16, 39, 25, 16, 31, 15, 27, 45, 34, 21, 21, 18, 25, 20, 35, 37, 9, 25, 48, 31, 33, 36, 35, 24, 29, 22, 34, 23, 36, 40, 21, 23, 22, 35, 25, 12, 19, 32, 80, 36, 33, 37, 34, 34, 31, 40, 34, 33, 25, 13, 59, 41, 35, 59, 30, 27, 18, 9, 28, 25, 20, 32, 35, 30, 47, 26, 35, 24, 37, 29, 22, 45, 25, 22, 35, 19, 30, 34, 26, 40, 35, 36, 21, 48, 33, 21, 23, 21, 40, 32, 13, 27, 12, 22, 27, 29, 23, 38, 35, 64, 27, 43, 48, 45, 38, 45, 52, 43, 27, 32, 14, 33, 12, 31, 26, 34, 16, 30, 38, 70, 28, 25, 40, 15, 40, 21, 31, 27, 25, 37, 46, 15, 20, 37, 74, 15, 22, 37, 23, 26, 33, 39, 8, 30, 29, 16, 26, 29, 17, 32, 37, 48, 27, 11, 27, 31, 33, 22, 29, 30, 21, 12, 33, 55, 24, 19, 36, 20, 28, 47, 14, 19, 16, 13, 43, 19, 25, 38, 35, 39, 14, 15, 41, 21, 33, 27, 20, 22, 78, 44, 16, 30, 44, 41, 30, 36, 13, 39, 32, 38, 35, 33, 42, 36, 44, 45, 46, 32, 24, 27, 31, 33, 39, 39, 17, 16, 41, 31, 30, 36, 27, 17, 23, 29, 18, 21, 30, 40, 30, 17, 15, 33, 24, 25, 21, 30, 16, 18, 50, 29, 11, 32, 35, 25, 15, 52, 18, 23, 22, 37, 43, 29, 39, 24, 34, 25, 33, 14, 37, 22, 16, 50, 74, 45, 44, 40, 43, 14, 26, 27, 45, 21, 32, 16, 50, 26, 20, 26, 26, 29, 50, 44, 75, 26, 50, 39, 55, 34, 23, 46, 51, 71, 30, 9, 39, 24, 65, 19, 31, 16, 12, 40, 48, 34, 17, 38, 39, 30, 31, 66, 30, 10, 23, 30, 34, 25, 25, 39, 32, 97, 91, 32, 70, 31, 35, 50, 19, 33, 34, 22, 44, 49, 29, 41, 15, 14, 54, 25, 51, 25, 29, 18, 76, 44, 13, 17, 56, 19, 41, 10, 20, 22, 18, 49, 12, 35, 23, 42, 41, 36, 46, 28, 66, 72, 27, 39, 29, 13, 59, 41, 20, 42, 19, 17, 62, 21, 52, 16, 22, 45, 72, 23, 17, 17, 17, 12, 73, 15, 50, 70, 46, 19, 50, 48, 31, 94, 7, 54, 54, 34, 44, 39, 40, 57, 31, 85, 20, 13, 18, 29, 22, 14, 43, 80, 15, 48, 10, 28, 46, 22, 46, 13, 17, 18, 69, 20, 14, 36, 41, 13, 35, 91, 39, 52, 18, 40, 16, 48, 75, 26, 28, 77, 17, 69, 13, 59, 50, 70, 27, 50, 13, 23, 23, 51, 23, 7, 16, 33, 21, 50, 60, 36, 44, 18, 17, 65, 15, 47, 8, 41, 22, 20, 48, 11, 17, 79, 31, 7, 71, 14, 29, 9, 17, 37, 61, 19, 34, 44, 41, 19, 31, 32, 31, 66, 30, 42, 56, 16, 39, 40, 36, 13, 30, 13, 56, 43, 39, 18, 49, 35, 49, 84, 92, 14, 16, 16, 9, 47, 24, 43, 18, 75, 9, 80, 23, 68, 50, 17, 32, 74, 66, 13, 39, 32, 56, 16, 26, 32, 20, 17, 81, 33, 39, 14, 17, 19, 65, 10, 38, 44, 25, 70, 29, 74, 36, 30, 142, 38, 6, 9, 11, 18, 48, 32, 66, 8, 25, 17, 86, 39, 17, 12, 32, 61, 20, 19, 45, 13, 21, 15, 22, 35, 23, 17, 23, 10, 42, 47, 67, 19, 31, 39, 13, 73, 15, 10, 46, 9, 6, 22, 23, 30, 16, 14, 34, 24, 40, 47, 36, 14, 27, 46, 39, 29, 24, 23, 34, 70, 66, 27, 43, 35, 13, 42, 25, 70, 14, 52, 38, 19, 35, 19, 20]\n",
      "Total word count across all overviews: 22691\n",
      "Number of unique words across all overviews: 8025\n",
      "Average frequency of each word across all overviews: 2.83\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total word count for each movie overview\n",
    "total_counts_per_overview = [sum(word_dict.values()) for word_dict in word_dicts]\n",
    "print(\"Total word count per overview:\", total_counts_per_overview)\n",
    "\n",
    "# Calculate the total number of words across all overviews\n",
    "total_word_count = sum(total_counts_per_overview)\n",
    "print(\"Total word count across all overviews:\", total_word_count)\n",
    "\n",
    "# Number of unique words across all overviews\n",
    "num_unique_words = len(cleaned_unique_words)\n",
    "print(\"Number of unique words across all overviews:\", num_unique_words)\n",
    "\n",
    "# Calculate the average frequency of each word across all overviews\n",
    "average_word_frequency = total_word_count / num_unique_words\n",
    "print(f\"Average frequency of each word across all overviews: {average_word_frequency:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis of the word frequency in the movie overviews reveals a comprehensive overview of word usage across the dataset. Each movie overview has a varying word count, indicating a diverse range of content lengths. The total word count across all overviews is $2,693$. This large count, combined with a unique word count of $8,026$, suggests a rich and varied vocabulary within the dataset.\n",
    "\n",
    "The average frequency of each word across all overviews is approximately $2.83$ times. This higher average indicates that some words are used more frequently than others, which is a crucial factor in the TF-IDF analysis. This variation in word frequency highlights the varied nature of the movie overviews, with certain terms likely holding more significance in specific overviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "7d69dd256522467bba77eeb0565dcb41",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movie overviews in the DataFrame: 693\n",
      "Preview of the DataFrame:\n",
      "   alesia  enthusiasts  everlasting  outfitted  periodically  almost  support  \\\n",
      "0       0            0            0          0             0       0        0   \n",
      "1       0            0            0          0             0       0        0   \n",
      "2       0            0            0          0             0       0        0   \n",
      "3       0            0            0          0             0       0        0   \n",
      "4       0            0            0          0             0       0        0   \n",
      "\n",
      "   investigations  ordinary  j  ...  allies  klebb  famed  realised  edgy  \\\n",
      "0               0         0  0  ...       0      0      0         0     0   \n",
      "1               0         0  0  ...       0      0      0         0     0   \n",
      "2               0         0  0  ...       0      0      0         0     0   \n",
      "3               0         0  0  ...       0      0      0         0     0   \n",
      "4               0         0  0  ...       0      0      0         0     0   \n",
      "\n",
      "   charlotte  perplexes  ridicules  comet  garden  \n",
      "0          0          0          0      0       0  \n",
      "1          0          0          0      0       0  \n",
      "2          0          0          0      0       0  \n",
      "3          0          0          0      0       0  \n",
      "4          0          0          0      0       0  \n",
      "\n",
      "[5 rows x 8025 columns]\n",
      "Unique word frequency counts across all movie overviews:\n",
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(word_dicts)\n",
    "\n",
    "print(f\"Number of movie overviews in the DataFrame: {len(df)}\")\n",
    "\n",
    "print(\"Preview of the DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Extract and print all unique frequency counts in the DataFrame\n",
    "unique_freq_counts = pd.Series(df.values.ravel()).unique()\n",
    "print(\"Unique word frequency counts across all movie overviews:\")\n",
    "print(unique_freq_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting our word frequency data into a structured format, we now have a DataFrame (`df`) where each row represents a movie overview and each column corresponds to a unique word.\n",
    "\n",
    "This analysis phase is essential in preparing us for the next steps, where we will calculate the TF-IDF scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step of our TF-IDF analysis, we focus on calculating the Term Frequencies (TF). Term frequency measures how frequently a term occurs in a movie overview. Since every movie overview is different in length, it is possible that a term would appear much more times in long movie overviews than shorter ones. Thus, the term frequency is often divided by the movie overview length (the total number of terms in the movie overview as a way of normalization:\n",
    "\n",
    "$$TF(t) = \\frac{\\text{Number of times term t appears in a movie overview}}{\\text{Total number of terms in the document}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "f390ac433e0b43149b5c9e83aa415bb6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "def computeTF(df):\n",
    "    # Ensure the DataFrame is in the correct data type for floating-point division\n",
    "    df = df.astype(float)\n",
    "    \n",
    "    # Initialize an empty DataFrame to hold the term frequencies\n",
    "    tf_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Calculate the total number of words in the document (row)\n",
    "        total_words = row.sum()\n",
    "        # print(\"Total words in document {}: {}\".format(index, total_words))\n",
    "        \n",
    "        # Copy the row to avoid modifying the original DataFrame\n",
    "        tf_row = row.copy()\n",
    "        \n",
    "        # Avoid division by zero by checking if total_words is not zero\n",
    "        if total_words != 0:\n",
    "            # Calculate the term frequency for each word\n",
    "            tf_row = row / total_words\n",
    "        else:\n",
    "            # If there are no words, set the term frequencies to zero\n",
    "            tf_row[:] = 0\n",
    "            # print(\"All term frequencies set to 0 for document {} because total_words is 0.\".format(index))\n",
    "        \n",
    "        # Append the row of term frequencies to the term frequency DataFrame\n",
    "        tf_df = tf_df.append(tf_row, ignore_index=True)\n",
    "        # print(\"tf_df after appending document {}:\\n{}\".format(index, tf_row))\n",
    "        \n",
    "    return tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the function, we apply it to our existing DataFrame (`df`) to compute the term frequencies. The resulting DataFrame, Term_Frequency_Data_Frame, is printed to give us an overview of the term frequencies across our dataset.\n",
    "\n",
    "This step is crucial as it lays the groundwork for the next phase of our analysis, where these term frequencies will be used to calculate the Inverse Document Frequency (IDF) and subsequently, the TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute term frequencies\n",
    "# Term_Frequency_Data_Frame = computeTF(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to the calculation of the Inverse Document Frequency (IDF), it's insightful to examine the term frequency data further. Specifically, we want to identify the non-zero term frequencies within our DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute IDF\n",
    "\n",
    "Our next step in the TF-IDF analysis is to calculate the Inverse Document Frequency (IDF). The IDF is a measure of how important a word is within a corpus. The goal of the IDF is to diminish the weight of terms that appear very frequently in the document set and increase the weight of terms that appear rarely.\n",
    "\n",
    "The IDF for a term is calculated as follows:\n",
    "\n",
    "$$IDF(t) = \\log\\left(\\frac{N}{\\texttt{df}(t)}\\right) + 1$$\n",
    "\n",
    "where:\n",
    "\n",
    "$N$ is the total number of movie overviews and $\\texttt{df}(t)$ is the number of movie overviews with term $t$ in it.\n",
    "\n",
    "We add $1$ to the $\\log$ term to smooth the IDF, preventing division by zero and ensuring that terms with zero frequency get a finite weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "27fa67ff4a814b75af22a5cbaf400a11",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "def compute_idf(tf_df):\n",
    "    # The number of documents\n",
    "    N = len(tf_df)\n",
    "    \n",
    "    # Counting the number of documents that contain each word\n",
    "    # Convert to binary to indicate presence or absence of a term\n",
    "    binary_tf = tf_df.gt(0).astype(int)\n",
    "    df = binary_tf.sum(axis=0)\n",
    "\n",
    "    # Apply the IDF formula with smoothing. Using log base 10\n",
    "    idf = np.log10((N + 1) / (df + 1)) + 1  # Added 1 to N and df to avoid division by zero\n",
    "    \n",
    "    # Converting to a DataFrame for easier handling\n",
    "    idf_df = pd.DataFrame(idf, index=tf_df.columns).rename(columns={0: 'IDF'})\n",
    "    \n",
    "    return idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the IDF\n",
    "# Inverse_Document_Frequency_Data_Frame = compute_idf(Term_Frequency_Data_Frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute TF-IDF scores\n",
    "\n",
    "Having computed both the Term Frequencies (TF) and the Inverse Document Frequencies (IDF), we now arrive at the final step of our TF-IDF analysis: calculating the TF-IDF scores. It is the product of TF and IDF, providing a weight to each word that signifies its relevance in the context of a specific movie overview as well as within the entire dataset.\n",
    "\n",
    "The TF-IDF score for a term in a document is calculated as follows:\n",
    "\n",
    "$$TF-IDF = TF(t, d) × IDF(t)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$TF(t, d)$ is the term frequency of term $t$ in the movie overview $d$\n",
    "$IDF(t)$ is the inverse document frequency of term $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "5ca8a2da186e42eaa20d0fffbb8c64ae",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "def compute_TFIDF(tf_df, idf_df):\n",
    "    # Ensure the column names match between the two DataFrames\n",
    "    tf_df.columns = idf_df.index\n",
    "    \n",
    "    # Multiply each TF row by the IDF vector\n",
    "    tfidf_df = tf_df * idf_df['IDF'].values\n",
    "    \n",
    "    return tfidf_df\n",
    "\n",
    "# Now call the function with your data\n",
    "# tfidf_df = compute_TFIDF(Term_Frequency_Data_Frame, Inverse_Document_Frequency_Data_Frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=699b63e8-cbf7-4458-8201-adc8db264c30' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a7a193055ef54954b9dd321898b559a1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
